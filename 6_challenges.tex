\section{Challenges and Future Directions}
\label{sec:challenges}

Integrating LLMs into phone automation has propelled significant advancements but also introduced numerous challenges. Overcoming these challenges is essential for fully unlocking the potential of intelligent phone GUI agents. This section outlines key issues and possible directions for future work, encompassing dataset development, scaling fine-tuning, lightweight on-device deployment, user-centric adaptation, improving model capabilities, standardizing benchmarks, and ensuring reliability and security.


\noindent\textbf{Dataset Development and Fine-Tuning Scalability.}
The performance of LLMs in phone automation heavily depends on datasets that capture diverse, real-world scenarios. Existing datasets often lack the breadth needed for comprehensive coverage. Future efforts should focus on developing large-scale, annotated datasets covering a wide range of applications, user behaviors, languages, and device types~\cite{rawles2024androidinthewild, zhang2024aitz}. Incorporating multimodal inputs—e.g., screenshots, UI trees, and natural language instructions—can help models better understand complex user interfaces. In addition, VideoGUI~\cite{lin2024videogui} proposes using instructional videos to demonstrate complex visual tasks to models, helping them to learn how to transition from an initial state to a target state. Video datasets are expected to evolve into a new form for future GUI datasets.
However, scaling fine-tuning to achieve robust out-of-domain performance remains a challenge. As shown by \textit{AndroidControl}~\cite{li2024androidcontrol}, obtaining reliable results for high-level tasks outside the training domain may require one to two orders of magnitude more data than currently feasible. Fine-tuning alone may not suffice. Future directions should explore hybrid training methodologies, unsupervised learning, transfer learning, and auxiliary tasks to improve generalization without demanding prohibitively large datasets.


\noindent\textbf{Lightweight and Efficient On-Device Deployment.}
Deploying LLMs on mobile devices confronts substantial computational and memory constraints. Current hardware often struggles to support large models with minimal latency and power consumption. Approaches such as model pruning, quantization, and efficient transformer architectures can address these constraints~\cite{ding2024mobileagentsop}. 
Recent innovations demonstrate promising progress. \textit{Octopus v2}~\cite{chen2024octopus} shows that a 2-billion parameter on-device model can outpace GPT-4 in accuracy and latency, while \textit{Lightweight Neural App Control}~\cite{christianos2024lightweight} achieves substantial speed and accuracy improvements by distributing tasks efficiently. \textit{AppVLM}~\cite{papoudakis2025appvlm}, a lightweight vision-language model, matches GPT-4o in online task completion success rate while being up to ten times faster, making it practical for real-world deployment. Moreover, specialized hardware accelerators and edge computing solutions can further reduce dependency on the cloud, enhance privacy, and improve responsiveness~\cite{wang2024mobileagentv2}. 
Consider leveraging the powerful code generation capabilities of small language models (SLMs) to transform GUI task automation into a code generation problem. This approach fully utilizes the strengths of SLMs, significantly enhancing the efficiency and performance of GUI agents on mobile devices~\cite{wen2024autodroidv2, wang2024comprehensive}.



\noindent\textbf{User-Centric Adaptation: Interaction and Personalization.}
Current agents often rely on extensive human intervention to correct errors or guide task execution, undermining seamless user experiences. Enhancing the agent's ability to understand user intent and reducing manual adjustments is crucial. Future research should improve natural language understanding, incorporate voice commands and gestures, and enable agents to learn continuously from user feedback~\cite{lee2023exploremobilegpt,wang2024mobileagentv1, wang2024mobileagentv2, citation-111}.
Personalization is equally important. One-size-fits-all solutions are insufficient given users' diverse preferences and usage patterns. Agents should quickly adapt to new tasks and user-specific contexts without costly retraining. Integrating manual teaching, zero-shot learning, and few-shot learning can help agents generalize from minimal user input~\cite{sodhi2024step, lee2023exploremobilegpt,song2024visiontasker,li2024uinav}, making them more flexible and universally applicable.
For example, AdaptAgent~\cite{verma2024adaptagent} is capable of adapting to entirely new domains with as few as two human demonstrations. This not only proves the efficiency of limited human input, but also paves a new path for the development of multi-modal agents with broad adaptability. Similarly, LearnAct~\cite{liu2025learnact} demonstrates the power of human demonstrations in mobile GUI agents, using a multi-agent framework to automatically extract knowledge from demonstrations to enhance task completion. It establishes demonstration-based learning as a promising direction for creating more personalized and adaptive mobile agents.



\noindent\textbf{Advancing Model Capabilities: Grounding, Reasoning, and Beyond.}
Accurately grounding language instructions in specific UI elements is a major hurdle. Although LLMs excel at language understanding, mapping instructions to precise UI interactions requires improved multimodal grounding. Future work should integrate advanced vision models, large-scale annotations, and more effective fusion techniques~\cite{gou2024navigating, cheng2024seeclick, you2024ferret, zhang2024ui-hawk}.
Beyond grounding, improving reasoning, long-horizon planning, and adaptability in complex scenarios remains essential. Agents must handle intricate workflows, interpret ambiguous instructions, and dynamically adjust strategies as contexts evolve. Achieving these goals will likely involve new architectures, memory mechanisms, and inference algorithms that extend beyond current LLM capabilities.


\noindent\textbf{Standardizing Evaluation Benchmarks.}
Objective and reproducible benchmarks are imperative for comparing model performance. Existing benchmarks often target narrow tasks or limited domains, complicating comprehensive evaluations. Unified benchmarks covering diverse tasks, app types, and interaction modalities would foster fair comparisons and encourage more versatile and robust solutions~\cite{wang2024mobileagentbench, xu2024androidlab, lu2024guiodyssey, rawles2024androidinthewild}.
These benchmarks should provide standardized metrics, scenarios, and evaluation protocols, enabling researchers to identify strengths, weaknesses, and paths for improvement with greater clarity.


\noindent\textbf{Ensuring Reliability and Security.}
As agents gain access to sensitive data and perform critical tasks, reliability and security are paramount. Current systems may be susceptible to adversarial attacks, data breaches, and unintended actions~\cite{wu2024adversarial}. At the same time, LLM agents are also susceptible to backdoor attacks~\cite{yang2024watch,wang2024badagent}. Recent research like AEIA-MN~\cite{chen2025aeia} has demonstrated that multimodal LLM-powered mobile agents are highly vulnerable to Active Environmental Injection Attacks, where attackers manipulate environmental elements (e.g., notifications) to mislead agents, achieving attack success rates up to 93\% in benchmark tests. Robust security protocols, error-handling techniques, and privacy-preserving methods are needed to protect user information and maintain user trust~\cite{ma2024coco,bai2024digirl}. Employing techniques such as data localization, encrypted communication, and anonymization can effectively protect user privacy while collecting data~\cite{wang2025fedmobileagent}.
Continuous monitoring and validation processes can detect vulnerabilities and mitigate risks in real-time~\cite{lee2023exploremobilegpt}. Ensuring that agents behave predictably, respect user privacy, and maintain consistent performance under challenging conditions will be crucial for widespread adoption and long-term sustainability.


Addressing these challenges involves concerted efforts in data collection, model training strategies, hardware optimization, user-centric adaptation, improved grounding and reasoning, standardized benchmarks, and strong security measures. By advancing these areas, the next generation of LLM-powered phone GUI agents can become more efficient, trustworthy, and capable, ultimately delivering seamless, personalized, and secure experiences for users in dynamic mobile environment.
