\section{Introduction}
\label{sec:introduction}
% \subsection{Phone Automation and Large Language Models}

\IEEEPARstart{T}{he} core of phone GUI automation involves programmatically simulating human interactions with mobile interfaces to accomplish complex tasks. This technology has wide applications in testing and shortcut creation, enhancing efficiency and reducing manual effort~\cite{azim2013targeted,pan2020reinforcement,koroglu2018qbe,li2019humanoid,degott2019learning}. Traditional approaches rely on predefined scripts and templates which, while functional, lack flexibility when confronting variable interfaces and dynamic environments~\cite{arnatovich2018systematic,deshmukh2023automated,nass2024overcoming,nass2021many,tramontana2019automated}.

In computer science, an agent perceives its environment through sensors and acts via actuators to achieve goals~\cite{li2024personal,guo2024large,wang2024survey,jin2024llms, bubeck2023sparks}. These range from simple scripts to complex systems capable of learning and adaptation~\cite{wang2024survey,jin2024llms,huang2024understanding}. Traditional phone automation agents are constrained by static scripts and limited adaptability, making them ill-suited for modern mobile interfaces' dynamic nature.

Building intelligent autonomous agents with planning, decision-making, and execution capabilities remains a long-term AI goal~\cite{albrecht2018autonomous}. As technologies advanced, agents evolved from traditional forms~\cite{anscombe2000intention,dennett1988precis,shoham1993agent} to AI agents~\cite{poole2010artificial,inkster2018empathy,gao2018neural} incorporating machine learning and probabilistic decision-making. However, these still struggle with complex instructions~\cite{luger2016like, amershi2014power} and dynamic environments~\cite{christiano2017deep, kohl2019mode}.

With the rapid development of Large Language Models (LLMs) like the GPT series~\cite{radford2018gpt1,radford2019gpt2,brown2020gpt3,achiam2023gpt} and specialized models such as Fuyu-8B~\cite{bavishi2023fuyu}, LLM-based agents have demonstrated powerful capabilities across numerous domains~\cite{wang2023voyager, hong2023metagpt, li2023camel, park2023generative, boiko2023emergent, qian2023communicative, xia2023towards, dasgupta2023collaborating, qian2024chatdev, dong2024self, goertzel2014artificial}. As Figure~\ref{fig:llm_vs_agent} illustrates, conversational LLMs primarily focus on language understanding and generation, while LLM-based agents extend these capabilities by integrating perception and action components. This integration enables interaction with external environments through multimodal inputs and operational outputs~\cite{wang2023voyager,hong2023metagpt,qian2024chatdev}, bridging language understanding and real-world interactions~\cite{xi2023rise,li2024personal,guo2024large,furuta2024exposing}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/llm_vs_agent.drawio.png}
    \caption{Comparison between conversational LLMs and phone GUI agents. While a conversational LLM can understand queries and provide informative responses (\textit{e.g.}, recommending coffee beans), a Phone GUI agent can go beyond text generation to perceive the device's interface, decide on an appropriate action (like tapping an app icon), and execute it in the real environment, thus enabling tasks like ordering a latte directly on the user's phone.}
    \label{fig:llm_vs_agent}
\end{figure}

Applying LLM-based agents to phone automation has created a new paradigm, making mobile interface operations more intelligent~\cite{hong2024cogagent,zheng2024gpt,zhang2023appagent,song2023navigating}. \textbf{\textit{LLM-powered phone GUI agents are intelligent systems that leverage large language models to understand, plan, and execute tasks on mobile devices by integrating natural language processing, multimodal perception, and action execution capabilities.}} These agents can recognize interfaces, understand instructions, perceive changes in real time, and respond dynamically. Unlike script-based automation, they can autonomously plan complex sequences through multimodal processing of instructions and interface information. Their adaptability and flexibility improve user experience through intent understanding, planning, and automated task execution, enhancing efficiency across scenarios from app testing to complex operations like configuring settings~\cite{wen2024autodroid}, navigating maps~\cite{wang2024mobileagentv1,wang2024mobileagentv2}, and shopping~\cite{zhang2023appagent}.


% \subsection{Motivation for Phone GUI agents Survey}
% Artificial General Intelligence (AGI) refers to the development of intelligent agents that possess the ability to understand, learn, and apply knowledge in a general, human-like manner across a wide range of tasks and domains\cite{goertzel2014artificial}. Achieving AGI has been a long-standing goal in the field of artificial intelligence. LLMs have revolutionized the agent field by introducing agents that leverage advanced understanding, reasoning, and multimodal perception capabilities, positioning them as "sparks" for the potential realization of AGI\cite{bubeck2023sparks}. LLM-powered phone GUI agents serve as a practical testing ground for AGI, as they integrate multiple advanced AI capabilities essential for developing autonomous and intelligent systems capable of operating effectively in complex and dynamic environment.


Clarifying the development trajectory of phone GUI agents is crucial. On one hand, with the support of large language models~\cite{radford2018gpt1,radford2019gpt2,brown2020gpt3,achiam2023gpt}, phone GUI agents can significantly enhance the efficiency of phone automation scenarios, making operations more intelligent and no longer limited to coding fixed operation paths. This enhancement not only optimizes phone automation processes but also expands the application scope of automation. On the other hand, phone GUI agents can understand and execute complex natural language instructions, transforming human intentions into specific operations such as automatically scheduling appointments, booking restaurants, summoning transportation, and even achieving functionalities similar to autonomous driving in advanced automation. These capabilities demonstrate the potential of phone GUI agents in executing complex tasks, providing convenience to users and laying practical foundations for AI development.

% Furthermore, the abilities of intention understanding, long-chain planning, and action execution exhibited by phone GUI agents are precisely what is needed in scenarios like embodied AI\cite{pfeifer2004embodied,duan2022survey}. In today's context where AGI has not been fully realized, in-depth exploration and research into these capabilities are particularly important, as they constitute crucial steps toward AGI. Therefore, understanding and advancing LLM-powered phone GUI agents not only benefit the field of phone automation but also contribute significantly to the broader pursuit of AGI.

With the increasing research on large language models in phone automation~\cite{wen2023droidbot,wen2024autodroid,wang2024mobileagentv1,wang2024mobileagentv2,liu2024vision,zhang2024mobileexperts,lu2024omniparser}, the research community's attention to this field has grown rapidly. However, there is still a lack of dedicated systematic surveys in this area, especially comprehensive explorations of phone automation from the perspective of large language models. Given the importance of phone GUI agents, the purpose of this paper is to fill this gap by systematically summarizing current research achievements, reviewing relevant literature, analyzing the application status of large language models in phone automation, and pointing out directions for future research.

% 后边等论文多起来在这里添加折线图，展示研究数量的变化
% as shown in Figure~\ref{fig:paper_nums}
% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/paper_nums.png}
%     \caption{Overview on LLM-powered phone GUI agents papers.}
%     \label{fig:paper_nums}
% \end{figure*}

To provide a comprehensive overview of the current state and future prospects of LLM-Powered GUI Agents in Phone Automation, we present a taxonomy that categorizes the field into three main areas: Frameworks of LLM-powered phone GUI agents, Large Language Models for Phone Automation, and Datasets and Evaluation Methods Figure~\ref{fig:phone_agent_taxonomy_final}. This taxonomy highlights the diversity and complexity of the field, as well as the interdisciplinary nature of the research involved.


% \input{figures/phone_agent_taxonomy}
\input{figures/phone_agent_taxonomy_final}


% \subsection{Contributions of This Paper}

Unlike previous literature reviews, which primarily focus on traditional phone automated testing methods, most existing surveys emphasize manual scripting or rule-based automation approaches without leveraging LLMs~\cite{arnatovich2018systematic,deshmukh2023automated,nass2024overcoming,nass2021many,tramontana2019automated}. These traditional methods face significant challenges in coping with dynamic changes, complex user interfaces, and the scalability required for modern applications. Although recent surveys have explored broader areas of multimodal agents and foundation models for GUI automation, such as \textit{Foundations and Recent Trends in Multimodal Mobile Agents: A Survey}~\cite{wu2024foundations}, \textit{GUI Agents with Foundation Models: A Comprehensive Survey}~\cite{wang2024gui}, and \textit{Large Language Model-Brained GUI Agents: A Survey}~\cite{zhang2024large}, these works primarily cover general GUI-based automation and multimodal applications.

However, a dedicated and focused survey on the role of large language models in phone GUI automation remains absent in the existing literature. This paper addresses the above-mentioned gap by systematically reviewing the latest developments, challenges, and opportunities in LLM-powered phone GUI agents, thereby offering a more targeted exploration of this emerging domain. Our main contributions can be summarized as follows:

\begin{itemize}
    \item \textbf{A Comprehensive and Systematic Survey of LLM-Powered Phone GUI Agents.} 
    We provide an in-depth and structured overview of recent literature on LLM-powered phone automation, examining its developmental trajectory, core technologies, and real-world application scenarios. By comparing LLM-driven methods to traditional phone automation approaches, this survey clarifies how large models transform GUI-based tasks and enable more intelligent, adaptive interaction paradigms.

    \item \textbf{Methodological Framework from Multiple Perspectives.} 
    Leveraging insights from existing studies, we propose a unified methodology for designing LLM-driven phone GUI agents. This encompasses framework design (e.g., single-agent vs.\ multi-agent vs.\ plan-then-act frameworks), LLM model selection and training (prompt engineering vs.\ training-based methods), data collection and preparation strategies (GUI-specific datasets and annotations), and evaluation protocols (benchmarks and metrics). Our systematic taxonomy and method-oriented discussion serve as practical guidelines for both academic and industrial practitioners.

    \item \textbf{In-Depth Analysis of Why LLMs Empower Phone Automation.}
    We delve into the fundamental reasons behind LLMs' capacity to enhance phone automation. By detailing their advancements in natural language comprehension, multimodal grounding, reasoning, and decision-making, we illustrate how LLMs bridge the gap between user intent and GUI actions. This analysis elucidates the critical role of large models in tackling issues of scalability, adaptability, and human-like interaction in real-world mobile environment.

    \item \textbf{Insights into Latest Developments, Datasets, and Benchmarks.}
    We introduce and evaluate the most recent progress in the field, highlighting innovative datasets that capture the complexity of modern GUIs and benchmarks that allow reliable performance assessment. These resources form the backbone of LLM-based phone automation, enabling systematic training, fair evaluation, and transparent comparisons across different agent designs.

    \item \textbf{Identification of Key Challenges and Novel Perspectives for Future Research.} 
    Beyond discussing mainstream hurdles (e.g., dataset coverage, on-device constraints, reliability), we propose forward-looking viewpoints on user-centric adaptations, security and privacy considerations, long-horizon planning, and multi-agent coordination. These novel perspectives shed light on how researchers and developers might advance the current state of the art toward more robust, secure, and personalized phone GUI agents.
\end{itemize}

By addressing these aspects, our survey not only provides an up-to-date map of LLM-powered phone GUI automation but also offers a clear roadmap for future exploration. We hope this work will guide researchers in identifying pressing open problems and inform practitioners about promising directions to harness LLMs in designing efficient, adaptive, and user-friendly phone GUI agents.
