\section{Introduction}
\label{sec:introduction}
% \subsection{Phone Automation and Large Language Models}

\IEEEPARstart{T}{he} core of phone GUI automation is to simulate human interactions with phone interfaces programmatically, thereby accomplishing a series of complex tasks. Phone automation is widely applied in areas such as application testing and shortcut instructions, aiming to enhance operational efficiency or free up human resource~\cite{azim2013targeted,pan2020reinforcement,koroglu2018qbe,li2019humanoid,degott2019learning}. Traditional phone automation often relies on predefined scripts and templates, which, while effective, tend to be rigid and inflexible when facing complex and variable user interfaces and dynamic environment~\cite{arnatovich2018systematic,deshmukh2023automated,nass2024overcoming,nass2021many,tramontana2019automated}. These methods can be viewed as early forms of agents, designed to perform specific tasks in a predetermined manner.

An agent, in the context of computer science and artificial intelligence, is an entity that perceives its environment through sensors and acts upon that environment through actuators to achieve specific goals~\cite{li2024personal,guo2024large,wang2024survey,jin2024llms, bubeck2023sparks}. Agents can range from simple scripts that execute fixed sequences of actions to complex systems capable of learning, reasoning, and adapting to new situations~\cite{wang2024survey,jin2024llms,huang2024understanding}. Traditional agents in phone automation are limited by their reliance on static scripts and lack of adaptability, making it challenging for them to handle the dynamic and complex nature of modern mobile interfaces.

Building intelligent autonomous agents with abilities in task planning, decision-making, and action execution has been a long-term goal of artificial intelligence~\cite{albrecht2018autonomous}. As artificial intelligence technologies have advanced, the development of agents has progressed from these traditional agents~\cite{anscombe2000intention,dennett1988precis,shoham1993agent} to AI agents~\cite{poole2010artificial,inkster2018empathy,gao2018neural} that incorporate machine learning and decision-making capabilities. These AI agents can learn from data, make decisions based on probabilistic models, and adapt to changes in the environment to some extent. However, they still face limitations in understanding complex user instructions~\cite{luger2016like, amershi2014power} and managing highly dynamic environment~\cite{christiano2017deep, kohl2019mode}.


With the rapid development of LLMs like the GPT series~\cite{radford2018gpt1,radford2019gpt2,brown2020gpt3,achiam2023gpt} and specialized models such as Fuyu-8B~\cite{bavishi2023fuyu}, agents based on these models have exhibited powerful capabilities in numerous fields~\cite{wang2023voyager, hong2023metagpt, li2023camel, park2023generative, boiko2023emergent, qian2023communicative, xia2023towards, dasgupta2023collaborating, qian2024chatdev, dong2024self, goertzel2014artificial}. As illustrated in Figure~\ref{fig:llm_vs_agent}, there are key differences between conversational LLMs and LLM-based agents. While conversational LLMs primarily focus on understanding and generating human language—engaging in dialogue, answering questions, summarizing information, and translating language—LLM-based agents extend beyond these capabilities by integrating perception and action components. This integration enables them to interact with the external environment through multimodal inputs, such as visual data from user interfaces, and perform actions that alter environmental states~\cite{wang2023voyager,hong2023metagpt,qian2024chatdev}. By combining perception, reasoning, and action, these agents can parse intricate instructions, formulate operational commands, and autonomously perform highly complex tasks, bridging the gap between language understanding and real-world interactions~\cite{xi2023rise,li2024personal,guo2024large,furuta2024exposing}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.99\linewidth]{figures/llm_vs_agent.drawio.png}
    \caption{Comparison between conversational LLMs and phone GUI agents. While a conversational LLM can understand queries and provide informative responses (\textit{e.g.}, recommending coffee beans), a Phone GUI agent can go beyond text generation to perceive the device’s interface, decide on an appropriate action (like tapping an app icon), and execute it in the real environment, thus enabling tasks like ordering a latte directly on the user’s phone.}
    \label{fig:llm_vs_agent}
\end{figure}

Applying LLM-based agents to phone automation has brought a new paradigm to traditional automation, making operations on phone interfaces more intelligent~\cite{hong2024cogagent,zheng2024gpt,zhang2023appagent,song2023navigating}. \textbf{\textit{LLM-powered phone GUI agents are intelligent systems that leverage large language models to understand, plan, and execute tasks on mobile devices by integrating natural language processing, multimodal perception, and action execution capabilities.}} On smartphones, these agents can recognize and analyze user interfaces, understand natural language instructions, perceive interface changes in real time, and respond dynamically. Unlike traditional script-based automation that relies on coding fixed operation paths, these agents can autonomously plan complex task sequences through multimodal processing of language instructions and interface information. They have strong adaptability and flexible pathways, greatly improving user experience by understanding human intentions, performing complex long-chain planning, and executing tasks automatically, thereby improving efficiency in a wide range of scenarios, including not only phone automated testing but also executing complex tasks such as configuring intricate phone settings~\cite{wen2024autodroid}, navigating maps~\cite{wang2024mobileagentv1,wang2024mobileagentv2}, and facilitating online shopping~\cite{zhang2023appagent}.


% \subsection{Motivation for Phone GUI agents Survey}
% Artificial General Intelligence (AGI) refers to the development of intelligent agents that possess the ability to understand, learn, and apply knowledge in a general, human-like manner across a wide range of tasks and domains\cite{goertzel2014artificial}. Achieving AGI has been a long-standing goal in the field of artificial intelligence. LLMs have revolutionized the agent field by introducing agents that leverage advanced understanding, reasoning, and multimodal perception capabilities, positioning them as "sparks" for the potential realization of AGI\cite{bubeck2023sparks}. LLM-powered phone GUI agents serve as a practical testing ground for AGI, as they integrate multiple advanced AI capabilities essential for developing autonomous and intelligent systems capable of operating effectively in complex and dynamic environment.


Clarifying the development trajectory of phone GUI agents is crucial. On one hand, with the support of large language models~\cite{radford2018gpt1,radford2019gpt2,brown2020gpt3,achiam2023gpt}, phone GUI agents can significantly enhance the efficiency of phone automation scenarios, making operations more intelligent and no longer limited to coding fixed operation paths. This enhancement not only optimizes phone automation processes but also expands the application scope of automation. On the other hand, phone GUI agents can understand and execute complex natural language instructions, transforming human intentions into specific operations such as automatically scheduling appointments, booking restaurants, summoning transportation, and even achieving functionalities similar to autonomous driving in advanced automation. These capabilities demonstrate the potential of phone GUI agents in executing complex tasks, providing convenience to users and laying practical foundations for AI development.

% Furthermore, the abilities of intention understanding, long-chain planning, and action execution exhibited by phone GUI agents are precisely what is needed in scenarios like embodied AI\cite{pfeifer2004embodied,duan2022survey}. In today's context where AGI has not been fully realized, in-depth exploration and research into these capabilities are particularly important, as they constitute crucial steps toward AGI. Therefore, understanding and advancing LLM-powered phone GUI agents not only benefit the field of phone automation but also contribute significantly to the broader pursuit of AGI.

With the increasing research on large language models in phone automation~\cite{wen2023droidbot,wen2024autodroid,wang2024mobileagentv1,wang2024mobileagentv2,liu2024vision,zhang2024mobileexperts,lu2024omniparser}, the research community's attention to this field has grown rapidly. However, there is still a lack of dedicated systematic surveys in this area, especially comprehensive explorations of phone automation from the perspective of large language models. Given the importance of phone GUI agents, the purpose of this paper is to fill this gap by systematically summarizing current research achievements, reviewing relevant literature, analyzing the application status of large language models in phone automation, and pointing out directions for future research.

% 后边等论文多起来在这里添加折线图，展示研究数量的变化
% as shown in Figure~\ref{fig:paper_nums}
% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/paper_nums.png}
%     \caption{Overview on LLM-powered phone GUI agents papers.}
%     \label{fig:paper_nums}
% \end{figure*}

To provide a comprehensive overview of the current state and future prospects of LLM-Powered GUI Agents in Phone Automation, we present a taxonomy that categorizes the field into three main areas: Frameworks of LLM-powered phone GUI agents, Large Language Models for Phone Automation, and Datasets and Evaluation Methods Figure~\ref{fig:phone_agent_taxonomy_final}. This taxonomy highlights the diversity and complexity of the field, as well as the interdisciplinary nature of the research involved.


% \input{figures/phone_agent_taxonomy}
\input{figures/phone_agent_taxonomy_final}


% \subsection{Contributions of This Paper}

Unlike previous literature reviews, which primarily focus on traditional phone automated testing methods, most existing surveys emphasize manual scripting or rule-based automation approaches without leveraging LLMs~\cite{arnatovich2018systematic,deshmukh2023automated,nass2024overcoming,nass2021many,tramontana2019automated}. These traditional methods face significant challenges in coping with dynamic changes, complex user interfaces, and the scalability required for modern applications. Although recent surveys have explored broader areas of multimodal agents and foundation models for GUI automation, such as \textit{Foundations and Recent Trends in Multimodal Mobile Agents: A Survey}~\cite{wu2024foundations}, \textit{GUI Agents with Foundation Models: A Comprehensive Survey}~\cite{wang2024gui}, and \textit{Large Language Model-Brained GUI Agents: A Survey}~\cite{zhang2024large}, these works primarily cover general GUI-based automation and multimodal applications.

However, a dedicated and focused survey on the role of large language models in phone GUI automation remains absent in the existing literature. This paper addresses the above-mentioned gap by systematically reviewing the latest developments, challenges, and opportunities in LLM-powered phone GUI agents, thereby offering a more targeted exploration of this emerging domain. Our main contributions can be summarized as follows:

\begin{itemize}
    \item \textbf{A Comprehensive and Systematic Survey of LLM-Powered Phone GUI Agents.} 
    We provide an in-depth and structured overview of recent literature on LLM-powered phone automation, examining its developmental trajectory, core technologies, and real-world application scenarios. By comparing LLM-driven methods to traditional phone automation approaches, this survey clarifies how large models transform GUI-based tasks and enable more intelligent, adaptive interaction paradigms.

    \item \textbf{Methodological Framework from Multiple Perspectives.} 
    Leveraging insights from existing studies, we propose a unified methodology for designing LLM-driven phone GUI agents. This encompasses framework design (e.g., single-agent vs.\ multi-agent vs.\ plan-then-act frameworks), LLM model selection and training (prompt engineering vs.\ training-based methods), data collection and preparation strategies (GUI-specific datasets and annotations), and evaluation protocols (benchmarks and metrics). Our systematic taxonomy and method-oriented discussion serve as practical guidelines for both academic and industrial practitioners.

    \item \textbf{In-Depth Analysis of Why LLMs Empower Phone Automation.}
    We delve into the fundamental reasons behind LLMs' capacity to enhance phone automation. By detailing their advancements in natural language comprehension, multimodal grounding, reasoning, and decision-making, we illustrate how LLMs bridge the gap between user intent and GUI actions. This analysis elucidates the critical role of large models in tackling issues of scalability, adaptability, and human-like interaction in real-world mobile environment.

    \item \textbf{Insights into Latest Developments, Datasets, and Benchmarks.}
    We introduce and evaluate the most recent progress in the field, highlighting innovative datasets that capture the complexity of modern GUIs and benchmarks that allow reliable performance assessment. These resources form the backbone of LLM-based phone automation, enabling systematic training, fair evaluation, and transparent comparisons across different agent designs.

    \item \textbf{Identification of Key Challenges and Novel Perspectives for Future Research.} 
    Beyond discussing mainstream hurdles (e.g., dataset coverage, on-device constraints, reliability), we propose forward-looking viewpoints on user-centric adaptations, security and privacy considerations, long-horizon planning, and multi-agent coordination. These novel perspectives shed light on how researchers and developers might advance the current state of the art toward more robust, secure, and personalized phone GUI agents.
\end{itemize}

By addressing these aspects, our survey not only provides an up-to-date map of LLM-powered phone GUI automation but also offers a clear roadmap for future exploration. We hope this work will guide researchers in identifying pressing open problems and inform practitioners about promising directions to harness LLMs in designing efficient, adaptive, and user-friendly phone GUI agents.
